{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15stoH2Yq1I1Q7rbHQcFr4cWVNBukPZsO",
      "authorship_tag": "ABX9TyO2yNTU1EhdSRTjB2t+Xc+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniket-desai-24/Machine-Learning/blob/main/Sentiment%20Analysis%20of%20Extracted%20Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Analysis: Sentiment Analysis of Extracted Articles**\n"
      ],
      "metadata": {
        "id": "scvQqUOpUpQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The objective of this assignment is to extract textual data articles from a given URL and perform text analysis to compute variables. The process involves data extraction and data analysis ,explain methodology adopted to perform text analysis to drive sentimental opinion, sentiment scores, readability, passive words, personal pronouns and etc."
      ],
      "metadata": {
        "id": "uMC6c7BhUO3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qIKpP6m6yA2",
        "outputId": "a16b51bb-c361-4e0b-dab6-8a2356b7767e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Tasks/20211030 Test Assignment\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Tasks/20211030 Test Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet imports libraries such as requests, BeautifulSoup, pandas, nltk, and re. These libraries are commonly used for web scraping, text analysis, and data manipulation tasks. The code also downloads NLTK's tokenization and stopwords resources. The purpose of the code seems to involve extracting data from web pages, preprocessing the text using NLTK and regular expressions, and potentially utilizing Pandas for data storage and manipulation."
      ],
      "metadata": {
        "id": "-Z4vS4DXWZF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUBpWagS7xRG",
        "outputId": "a864e307-19f4-49ce-83e1-525efedc4a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code reads URLs from an Excel file, extracts article titles and text from the web pages using BeautifulSoup, and saves them in separate text files. It handles errors during the extraction process and continues to the next URL. However, there are some URLs (ID 44, 57, 144) that couldn't be found on the website."
      ],
      "metadata": {
        "id": "sldrHM10XKhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read the url file into the pandas object to do further opration.\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "#loop throgh each row in the df\n",
        "for index, row in df.iterrows():\n",
        "  url = row['URL']\n",
        "  url_id = row['URL_ID']\n",
        "\n",
        "  # make a request to url to Access tha Data.\n",
        "  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
        "  try:\n",
        "    response = requests.get(url,headers=header)\n",
        "  except:\n",
        "    print(\"can't get response of {}\".format(url_id))\n",
        "\n",
        "  #create a beautifulsoup object\n",
        "\n",
        "  try:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  except:\n",
        "    print(\"can't get page of {}\".format(url_id))\n",
        "\n",
        "\n",
        "  #find title\n",
        "  try:\n",
        "    title = soup.find('h1').get_text()\n",
        "  except:\n",
        "    print(\"can't get title of {}\".format(url_id))\n",
        "    continue\n",
        "\n",
        "\n",
        "  #find text in this most of the site have same tag for article text but some of them are in different tag so i have used both.\n",
        "\n",
        "  article_text = \"\"\n",
        "\n",
        "  all_text=soup.find('div',class_=\"td-post-content tagdiv-type\")\n",
        "\n",
        "\n",
        "  if all_text:\n",
        "    try:\n",
        "\n",
        "      for p in all_text.find_all('p'):\n",
        "        article_text += p.get_text()\n",
        "\n",
        "    except:\n",
        "      print(\"can't get text of {}\".format(url_id))\n",
        "      continue\n",
        "\n",
        "  else :\n",
        "    remain_text=soup.find_all('div',class_=\"tdb-block-inner td-fix-index\")\n",
        "    remain_text=remain_text[14]\n",
        "    try:\n",
        "\n",
        "      for p in remain_text.find_all('p'):\n",
        "        article_text += p.get_text()\n",
        "\n",
        "    except:\n",
        "      print(\"can't get text of {}\".format(url_id))\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "  #write title and text to the file\n",
        "  file_name = '/content/drive/MyDrive/Tasks/20211030 Test Assignment/TitleText/' + str(url_id) + '.txt'\n",
        "  with open(file_name, 'w') as file:\n",
        "    file.write(title + '\\n' + article_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpe1kZN08Cij",
        "outputId": "8f245d5b-b8f1-48d2-a5e0-f02f6f197163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can't get title of 44\n",
            "can't get title of 57\n",
            "can't get title of 144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code snippet defines three directories:**\n",
        "\n",
        "1)text_dir: This directory is used for storing the extracted article text files.\n",
        "\n",
        "2)stopwords_dir: This directory is likely used for storing stop words, which are common words excluded during text analysis.\n",
        "\n",
        "3)sentment_dir: This directory may contain a master dictionary or lexicon for sentiment analysis.\n",
        "These directories are essential for organizing the output files, managing stop words, and accessing the sentiment dictionary in the provided code."
      ],
      "metadata": {
        "id": "lk9euazFZ4o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories\n",
        "text_dir = \"/content/drive/MyDrive/Tasks/20211030 Test Assignment/TitleText\"\n",
        "stopwords_dir = \"/content/drive/MyDrive/Tasks/20211030 Test Assignment/StopWords\"\n",
        "sentment_dir = \"/content/drive/MyDrive/Tasks/20211030 Test Assignment/MasterDictionary\""
      ],
      "metadata": {
        "id": "tWgHTmd_cWhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads stop words from files in the stopwords_dir directory and adds them to a set called stop_words. It iterates over each file, reads its content, and updates the set by adding the stop words from each file. This ensures that the stop_words set contains a consolidated set of stop words from all the files in the directory."
      ],
      "metadata": {
        "id": "kvJlDjN7aEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all stop words from the stopwords directory and store in the stop_words set variable\n",
        "stop_words = set()\n",
        "for filename in os.listdir(stopwords_dir):\n",
        "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "        stop_words.update(set(f.read().splitlines()))"
      ],
      "metadata": {
        "id": "6PvPUPQxcWkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code loads text files, tokenizes their content, removes stop words, and stores the filtered tokens in the docs list. It performs these steps: initializes an empty list called docs, iterates over each file in the text_dir directory, reads the file content, tokenizes it, removes stop words, and appends the filtered tokens to the docs list."
      ],
      "metadata": {
        "id": "LxKYSSOOa4Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all text files from the text directory and store in a list (docs)\n",
        "docs = []\n",
        "for text_file in os.listdir(text_dir):\n",
        "    file_path = os.path.join(text_dir, text_file)\n",
        "    if os.path.isdir(file_path):  # Skip directories\n",
        "        continue\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "        # Tokenize the given text file\n",
        "        words = word_tokenize(text)\n",
        "        # Remove the stop words from the tokens\n",
        "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "        # Add each filtered token of each file into a list\n",
        "        docs.append(filtered_text)"
      ],
      "metadata": {
        "id": "OlsWnzUNcWnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code loads positive and negative words from files in the sentment_dir directory and stores them in separate sets, pos and neg. It reads the content of each file and adds the words to the corresponding set. The sets can be used for sentiment analysis to determine the sentiment polarity of text data."
      ],
      "metadata": {
        "id": "t4do1eyXb-wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store positive and negative words from the directory\n",
        "pos = set()\n",
        "neg = set()\n",
        "\n",
        "for files in os.listdir(sentment_dir):\n",
        "    if files == 'positive-words.txt':\n",
        "        with open(os.path.join(sentment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
        "            pos.update(f.read().splitlines())\n",
        "    else:\n",
        "        with open(os.path.join(sentment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
        "            neg.update(f.read().splitlines())"
      ],
      "metadata": {
        "id": "47TY3V6LcWqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code calculates sentiment-related variables for each document in the docs list. It collects positive and negative words, calculates positive and negative scores, and derives polarity and subjectivity scores. The results are stored in separate lists for further analysis."
      ],
      "metadata": {
        "id": "p15DQGFfcu0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists for positive and negative words, scores, and polarity/subjectivity scores\n",
        "positive_words = []\n",
        "negative_words = []\n",
        "positive_score = []\n",
        "negative_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "\n",
        "# Iterate through the list of docs\n",
        "for doc in docs:\n",
        "\n",
        "    # Collect the positive and negative words from each document\n",
        "    positive_words.append([word for word in doc if word.lower() in pos])\n",
        "    negative_words.append([word for word in doc if word.lower() in neg])\n",
        "\n",
        "    # Calculate the scores from the positive and negative words\n",
        "    positive_score.append(len(positive_words[-1]))\n",
        "    negative_score.append(len(negative_words[-1]))\n",
        "    polarity_score.append((positive_score[-1] - negative_score[-1]) / ((positive_score[-1] + negative_score[-1]) + 0.000001))\n",
        "    subjectivity_score.append((positive_score[-1] + negative_score[-1]) / (len(doc) + 0.000001))\n"
      ],
      "metadata": {
        "id": "IGN3apPN-xJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code calculates various text analysis variables for each file in the text_dir directory. It calculates the average sentence length, percentage of complex words, Fog Index, count of complex words, and average syllable count per word. The results are stored in separate lists for further analysis."
      ],
      "metadata": {
        "id": "aDmWtL4xe8hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sentence_length = []\n",
        "Percentage_of_Complex_words = []\n",
        "Fog_Index = []\n",
        "complex_word_count = []\n",
        "avg_syllable_word_count = []\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "def measure(file):\n",
        "    with open(os.path.join(text_dir, file), 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "\n",
        "    # split the given text file into sentences\n",
        "    sentences = text.split('.')\n",
        "\n",
        "    # total number of sentences in a file\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    # total words in the file\n",
        "    words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "    num_words = len(words)\n",
        "\n",
        "    # complex words having syllable count is greater than 2\n",
        "    complex_words = []\n",
        "    for word in words:\n",
        "        vowels = 'aeiou'\n",
        "        syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
        "        if syllable_count_word > 2:\n",
        "            complex_words.append(word)\n",
        "\n",
        "    # Syllable Count Per Word\n",
        "    syllable_count = 0\n",
        "    syllable_words = []\n",
        "    for word in words:\n",
        "        if word.endswith('es'):\n",
        "            word = word[:-2]\n",
        "        elif word.endswith('ed'):\n",
        "            word = word[:-2]\n",
        "        vowels = 'aeiou'\n",
        "        syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
        "        if syllable_count_word >= 1:\n",
        "            syllable_words.append(word)\n",
        "            syllable_count += syllable_count_word\n",
        "\n",
        "    avg_sentence_len = num_words / num_sentences\n",
        "    avg_syllable_word_count = syllable_count / len(syllable_words)\n",
        "    Percent_Complex_words = len(complex_words) / num_words\n",
        "    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
        "\n",
        "    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n",
        "\n",
        "# iterate through each file or doc\n",
        "for file in os.listdir(text_dir):\n",
        "\n",
        "    # Skip directories this is used beacuse of the .ipynb_Ckeckpoint directory\n",
        "    if os.path.isdir(os.path.join(text_dir, file)):\n",
        "        continue\n",
        "\n",
        "    AS, PCW, FI, CWC, ASW = measure(file)\n",
        "    avg_sentence_length.append(AS)\n",
        "    Percentage_of_Complex_words.append(PCW)\n",
        "    Fog_Index.append(FI)\n",
        "    complex_word_count.append(CWC)\n",
        "    avg_syllable_word_count.append(ASW)\n"
      ],
      "metadata": {
        "id": "3Kd9B1vvMzmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code calculates the word count and average word length for each file in the text_dir directory. It removes punctuations and stop words from the text, and then calculates the word count by counting the cleaned words and the average word length by dividing the total length of words by the number of cleaned words. The results are stored in the word_count and average_word_length lists, respectively."
      ],
      "metadata": {
        "id": "L4AJYX7NhOxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n",
        "# We count the total cleaned words present in the text by\n",
        "# removing the stop words (using stopwords class of nltk package).\n",
        "# removing any punctuations like ? ! , . from the word before counting.\n",
        "\n",
        "word_count = []\n",
        "average_word_length = []\n",
        "\n",
        "def cleaned_words(file):\n",
        "  with open(os.path.join(text_dir,file), 'r') as f:\n",
        "    text = f.read()\n",
        "    text = re.sub(r'[^\\w\\s]', '' , text)\n",
        "    words = [word  for word in text.split() if word.lower() not in stopwords]\n",
        "    length = sum(len(word) for word in words)\n",
        "    average_word_length = length / len(words)\n",
        "  return len(words),average_word_length\n",
        "\n",
        "\n",
        "for file in os.listdir(text_dir):\n",
        "  # Skip directories this is used beacuse of the .ipynb_Ckeckpoint directory\n",
        "    if os.path.isdir(os.path.join(text_dir, file)):\n",
        "        continue\n",
        "    x, y = cleaned_words(file)\n",
        "    word_count.append(x)\n",
        "    average_word_length.append(y)"
      ],
      "metadata": {
        "id": "cUhise5Xeoql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code counts the occurrences of specific personal pronouns (\"I,\" \"we,\" \"my,\" \"ours,\" and \"us\") in each file within the text_dir directory. It stores the counts in a list called pp_count for further analysis."
      ],
      "metadata": {
        "id": "pfB0FrYGhR_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate Personal Pronouns mentioned in the text, we use regex to find\n",
        "# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n",
        "#  so that the country name US is not included in the list.\n",
        "\n",
        "\n",
        "def count_personal_pronouns(file):\n",
        "  with open(os.path.join(text_dir,file), 'r') as f:\n",
        "    text = f.read()\n",
        "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "    count = 0\n",
        "    for pronoun in personal_pronouns:\n",
        "      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
        "  return count\n",
        "\n",
        "pp_count = []\n",
        "for file in os.listdir(text_dir):\n",
        "  if os.path.isdir(os.path.join(text_dir, file)):\n",
        "        continue\n",
        "  x = count_personal_pronouns(file)\n",
        "  pp_count.append(x)"
      ],
      "metadata": {
        "id": "_sBkPF-8eota"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code reads an Excel file called \"Output Data Structure.xlsx\" into a DataFrame named output_df. It drops specific rows from the DataFrame where the URL_ID corresponds to non-existing pages.\n",
        "\n",
        "The code then defines a list of variables that include the calculated analysis parameters. It iterates over each variable and assigns its values to the corresponding column in the output_df DataFrame.\n",
        "\n",
        "Finally, the updated DataFrame is saved as a CSV file named \"Output_Data.csv\".\n",
        "\n",
        "In summary, the code updates and saves the output_df DataFrame with the calculated variables obtained from the analysis."
      ],
      "metadata": {
        "id": "5TvNwzN3h_LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
        "\n",
        "# URL_ID 44 ,57, 144 does not exists i,e. page does not exist, throughs 404 error\n",
        "# so we are going to drop these rows from the table\n",
        "output_df.drop([44-37,57-37,144-37], axis = 0, inplace=True)\n",
        "\n",
        "# These are the required parameters\n",
        "variables = [positive_score,\n",
        "            negative_score,\n",
        "            polarity_score,\n",
        "            subjectivity_score,\n",
        "            avg_sentence_length,\n",
        "            Percentage_of_Complex_words,\n",
        "            Fog_Index,\n",
        "            avg_sentence_length,\n",
        "            complex_word_count,\n",
        "            word_count,\n",
        "            avg_syllable_word_count,\n",
        "            pp_count,\n",
        "            average_word_length]\n",
        "\n",
        "# write the values to the dataframe\n",
        "for i, var in enumerate(variables):\n",
        "  output_df.iloc[:,i+2] = var\n",
        "\n",
        "#now save the dataframe to the disk\n",
        "output_df.to_csv('Output_Data.csv',index=False)"
      ],
      "metadata": {
        "id": "QRZTIUtpeowS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}